---
title: "All Wrecks Modeling"
format: html
editor: source
---

# Data Collection

## Reading data + initial packages

```{r}
# reading in data
library(readr)
data <- read.csv("sink.csv")
data <- data[,-1]
data$sank <- factor(data$sank)
data$sank <- factor(data$sank, levels = c(0, 1), labels = c("Not", "Sank"))
levels(data$sank) <- make.names(levels(data$sank), unique = TRUE)

library(glmnet)
library(tidyverse)
library(dplyr)
library(ISLR)
library(leaps)
library(caret)
library(mlbench)
library(pROC)
library(e1071)
library(cvms)
library(tibble) 
library(ggplot2)
library(ggimage)
```

## Splitting data
```{r}
set.seed(23)
train_index <- sample(nrow(data), nrow(data) * 0.7)
train <- data[train_index, ]
test <- data[-train_index, ]
```

# Logistic Classification

Note: We don't expect this to work, due to such high multicolinearity in the data.

```{r}
# logistic -- doesn't work due to high multicolinearity
lrFit <- glm(sank ~ ., data = train, family = "binomial") # does not converge
predictions <- predict(lrFit, newdata = test, type = "response", allowRankDeficiency = TRUE)

# Compute accuracy for different thresholds
thresholds <- seq(0.01, 0.99, by = 0.001)
accuracy <- rep(0, length(thresholds))
for (i in 1:length(thresholds)) {
  binary_predictions <- ifelse(predictions >= thresholds[i], "Sank", "Not")
  confusion_matrix <- table(test$sank, binary_predictions)
  accuracy[i] <- sum(diag(confusion_matrix))/sum(confusion_matrix)
}

# Find the threshold with the highest accuracy
optimal_threshold <- thresholds[which.max(accuracy)]
print(paste0("Optimal threshold: ", round(optimal_threshold, 3)))

# Convert the predictions to binary using a threshold of 0.5
binary_predictions <- ifelse(predictions >= optimal_threshold, "Sank", "Not")

# Evaluate the performance of the model
logit_confusion_matrix <- table(test$sank, binary_predictions)
logit_accuracy <- sum(diag(logit_confusion_matrix))/sum(logit_confusion_matrix)
print(logit_confusion_matrix)
print(logit_accuracy)
```

```{r}
# Convert confusion matrix to data frame
conf_df <- as.data.frame(logit_confusion_matrix)
names(conf_df) <- c("Predicted", "Actual", "Count")

# Create additional rows
additional <- data.frame(Predicted = c("Sank", "Not"),
                         Actual = c("Sank", "Sank"),
                         Count = c(0, 0))

# Combine the two dataframes
conf_df <- rbind(conf_df, additional)
names(conf_df) <- c("Predicted", "Actual", "Count")
print(conf_df)

logit_accuracy <- (conf_df$Count[1] + conf_df$Count[4]) / sum(conf_df$Count) * 100
logit_accuracy <- round(logit_accuracy, 3)
conf_df$Count <- factor(conf_df$Count, 
                        levels = c(conf_df$Count[1], 
                                   0, 
                                   conf_df$Count[2]))

conf_df$Predicted <- factor(conf_df$Predicted, levels = c("Sank", "Not"))

ggplot(data = conf_df, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 11, family = "Optima") +
  scale_fill_manual(values = c("#65749D", "#a1a1a1", "#D9D9D9", "#a1a1a1")) +
  labs(title = "\n Logistic Classification Confusion Matrix", 
       x = "Predicted", 
       y = "Actual", 
       fill = "", 
       subtitle = paste0("Accuracy: ", round(svm_accuracy,2), "%\nAUC: N/A due to the model classifying all values as 'Not'")) +
  theme_transparent()  +
  theme(text = element_text(family = "Optima", size = 14),
        plot.subtitle = element_text(size = 12, hjust = 0.5),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "#e7f7fa")) +
  guides(fill = 'none')
```

It predicted every row as sank. While a 99.59% accuracy may seem good, for our imbalanced data it is very much not. If anything, it has become the standard that we will hold all other models to.

# Decision Tree
```{r}
# Set up a grid of hyperparameters to tune for the decision tree model
dtGrid <- expand.grid(cp = seq(0.01, 0.5, by = 0.01))

# Train the decision tree model and cross-validate
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
dtFit <- train(sank ~ ., data = train[,1:44], method = "rpart", trControl = ctrl, tuneGrid = dtGrid)

# Make predictions on the test set and calculate the AUC
pred_dt <- predict(dtFit, newdata = test, type = "prob")[, 2]
roc_dt <- roc(test$sank, pred_dt)

# Find the optimal threshold based on the maximum Youden's J statistic
J <- roc_dt$specificities + roc_dt$sensitivities - 1
opt_thresh_dt <- roc_dt$thresholds[which.max(J)]

# Make binary predictions using the optimal threshold
pred_dt_bin <- as.numeric(pred_dt > opt_thresh_dt)
pred_dt_bin <- ifelse(pred_dt_bin == 0, "Not", "Sank")
```

```{r}
# Confusion Matrix
library(ggimage)

predicted <- pred_dt_bin
actual <- test$sank

# Create confusion matrix with predicted and actual numbers
conf_mat <- table(predicted, actual)

# Convert confusion matrix to data frame
conf_df <- as.data.frame(conf_mat)

names(conf_df) <- c("Predicted", "Actual", "Count")

dt_accuracy <- (conf_df$Count[1] + conf_df$Count[4]) / sum(conf_df$Count) * 100
dt_accuracy <- round(dt_accuracy, 3)
conf_df$Count <- factor(conf_df$Count, 
                        levels = c(conf_df$Count[1], 
                                   conf_df$Count[2], 
                                   conf_df$Count[3],
                                   conf_df$Count[4]))

conf_df$Predicted <- factor(conf_df$Predicted, levels = c("Sank", "Not"))
conf_df$Actual <- factor(conf_df$Actual, levels = c("Not", "Sank"))

dt_auc <- roc(ifelse(predicted == "Not", 0, 1), ifelse(actual == "Not", 0, 1))$auc

ggplot(data = conf_df, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 11, family = "Optima") +
  scale_fill_manual(values = c("#65749D", "#D9D9D9", "#D9D9D9", "#65749D")) +
  labs(title = "\nDecision Tree Confusion Matrix", 
       x = "Predicted", 
       y = "Actual", 
       fill = "", 
       subtitle = paste0("Accuracy: ", round(dt_accuracy, 3), "%\nAUC: ", round(dt_auc, 4))) +
  theme_transparent()  +
  theme(text = element_text(family = "Optima", size = 14),
        plot.subtitle = element_text(size = 12, hjust = 0.5),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "#e7f7fa")) +
  guides(fill = 'none') 

```

# Random Forest

## Setup
```{r}
## Random Forest

train_sample <- sample_n(train, 20000)

# split data into train, dev, and test sets
trainIndex <- createDataPartition(train_sample$sank, p = 0.7, list = FALSE)
train_data <- train_sample[trainIndex, ]
test_dev_data <- train_sample[-trainIndex, ]

devIndex <- createDataPartition(test_dev_data$sank, p = 0.5, list = FALSE)
dev_data <- test_dev_data[devIndex, ]
test_data <- test_dev_data[-devIndex, ]
```

## Testing on Sample
```{r}
# load libraries
library(randomForest)
library(caret)
library(purrr)

# define mtry and ntree values to try
mtry_values <- seq(2, 20, by = 2)
ntree_values <- seq(50, 250, by = 50)

# initialize matrices to store errors
train_errors <- matrix(NA, nrow = length(mtry_values), ncol = length(ntree_values))
dev_accuracy <- matrix(NA, nrow = length(mtry_values), ncol = length(ntree_values))

# set up progress bar
pb <- txtProgressBar(min = 0, max = length(mtry_values) * length(ntree_values), style = 3)

# loop over mtry and ntree values to train and evaluate models
counter <- 0
for (i in seq_along(mtry_values)) { 
  for (j in seq_along(ntree_values)) { 
    # train model
    rf_model <- randomForest(sank ~ ., 
                             data = train_data, 
                             mtry = mtry_values[i], 
                             ntree = ntree_values[j], 
                             do.trace = FALSE, 
                             importance = FALSE, 
                             proximity = FALSE, 
                             replace = TRUE)
    
    # calculate train error
    train_pred <- predict(rf_model, train_data)
    train_cm <- confusionMatrix(train_pred, train_data$sank)
    train_errors[i, j] <- train_cm$overall['Accuracy']
    
    # calculate dev error
    dev_pred <- predict(rf_model, dev_data)
    dev_cm <- confusionMatrix(dev_pred, dev_data$sank)
    dev_accuracy[i, j] <- dev_cm$overall['Accuracy'] 
    
    # update progress bar
    counter <- counter + 1
    setTxtProgressBar(pb, counter)
  } 
}

# remove progress bar
close(pb)

# find optimal parameters
max_dev_error <- max(dev_accuracy)
## is found at 
locations <- which(dev_accuracy == max(dev_accuracy), arr.ind = TRUE)[1,]

optimal_mtry <- mtry_values[locations[1]]
optimal_ntree <- ntree_values[locations[2]]

# print optimal parameters
cat(paste0("Optimal mtry: ", optimal_mtry, "\n")) # 4
cat(paste0("Optimal ntree: ", optimal_ntree, "\n")) # 50
```

## Bringing it to the full model

```{r}
## bringing it out to the entire testing dataset
rf_model <- randomForest(sank ~., 
                       data = train, 
                         mtry = 12,
                         ntree = 250)

test_predictions <- predict(rf_model, newdata = test)

dt_conf <- table(test_predictions, test$sank) %>% 
  as.data.frame() %>%
  as_tibble()

predicted <- test_predictions
actual <- test$sank
```

```{r}
# Get feature importances from random forest model
importances_rf <- importance(rf_model)

# Convert to data frame
importances_rf_df <- data.frame(Feature = row.names(importances_rf), Importance = importances_rf[,"MeanDecreaseGini"])

# Sort by importance
importances_rf_df <- importances_rf_df[order(importances_rf_df$Importance, decreasing = TRUE),]

write.csv(importances_rf_df, "rf_importances.csv")
```

```{r}
# Create confusion matrix with predicted and actual numbers
conf_mat <- table(predicted, actual)

# Convert confusion matrix to data frame
conf_df <- as.data.frame(conf_mat)

names(conf_df) <- c("Predicted", "Actual", "Count")

rf_accuracy <- (conf_df$Count[1] + conf_df$Count[4]) / sum(conf_df$Count) * 100
rf_accuracy <- round(rf_accuracy, 3)
conf_df$Count <- factor(conf_df$Count, 
                        levels = c(conf_df$Count[1], 
                                   conf_df$Count[2], 
                                   conf_df$Count[3],
                                   conf_df$Count[4]))

conf_df$Predicted <- factor(conf_df$Predicted, levels = c("Sank", "Not"))
conf_df$Actual <- factor(conf_df$Actual, levels = c("Not", "Sank"))

rf_auc <- roc(ifelse(test_predictions == "Not", 0, 1), ifelse(actual == "Not", 0, 1))$auc

ggplot(data = conf_df, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 11, family = "Optima") +
  scale_fill_manual(values = c("#65749D", "#D9D9D9", "#D9D9D9", "#65749D")) +
  labs(title = "\nRandom Forest Confusion Matrix", 
       x = "Predicted", 
       y = "Actual", 
       fill = "", 
       subtitle = paste0("Accuracy: ", rf_accuracy, "%\nAUC: ", round(rf_auc, 4))) +
  theme_transparent()  +
  theme(text = element_text(family = "Optima", size = 14),
        plot.subtitle = element_text(size = 12, hjust = 0.5),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "#e7f7fa")) +
  guides(fill = 'none') 

```

# SVM

```{r}
library(caret)
# Set up a grid of hyperparameters to tune for the SVM Linear model
svmGrid <- expand.grid(C = seq(0.01, 0.1, by = 0.01))

# Set up cross-validation method with 3 folds
ctrl <- trainControl(method = "cv", number = 3)

# Train the SVM Linear model and cross-validate
svmFit <- train(sank ~ ., 
                data = train[,1:44], 
                method = "svmLinear", 
                trControl = ctrl, 
                tuneGrid = svmGrid,
                preProc = c("center", "scale"))

# Get predictions on the test data
svm_pred <- predict(svmFit, newdata = test[,1:44])
```

```{r}
predicted <- svm_pred
actual <- test$sank

# Create confusion matrix with predicted and actual numbers
conf_mat <- table(predicted, actual)

# Convert confusion matrix to data frame
conf_df <- as.data.frame(conf_mat)

names(conf_df) <- c("Predicted", "Actual", "Count")

svm_accuracy <- (conf_df$Count[1] + conf_df$Count[4]) / sum(conf_df$Count) * 100
svm_accuracy <- round(svm_accuracy, 3)
conf_df$Count <- factor(conf_df$Count, 
                        levels = c(conf_df$Count[1], 
                                   0, 
                                   conf_df$Count[3]))

conf_df$Predicted <- factor(conf_df$Predicted, levels = c("Sank", "Not"))
conf_df$Actual <- factor(conf_df$Actual, levels = c("Not", "Sank"))

predicted <- factor(svm_pred, levels = c("Sank", "Not")) %>% as.numeric()
actual <- factor(test$sank, levels = c("Sank", "Not")) %>% as.numeric()

# svm_auc <- roc(predicted, actual)$auc

ggplot(data = conf_df, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 11, family = "Optima") +
  scale_fill_manual(values = c("#65749D", "#a1a1a1", "#D9D9D9", "#a1a1a1")) +
  labs(title = "\nLogistic + SVM Confusion Matrix", 
       x = "Predicted", 
       y = "Actual", 
       fill = "", 
       subtitle = paste0("Accuracy: ", round(svm_accuracy,2), "%\nAUC: N/A due to both models classifying all values as 'Not'")) +
  theme_transparent()  +
  theme(text = element_text(family = "Optima", size = 14),
        plot.subtitle = element_text(size = 12, hjust = 0.5),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "#e7f7fa")) +
  guides(fill = 'none')
```

# Neural Network Confusion Matrices
## One Layer
```{r}
one_layer <- data.frame(Predicted = c("Not", "Not", "Sank", "Sank"),
                        Actual = c("Not", "Sank", "Not", "Sank"),
                        Count = c(37948, 31, 73, 102))

one_layer_accuracy <- (one_layer$Count[1] + one_layer$Count[4]) / sum(one_layer$Count) * 100
one_layer_accuracy <- round(one_layer_accuracy, 3)
one_layer$Count <- factor(one_layer$Count, 
                        levels = c(one_layer$Count[1], 
                                   one_layer$Count[2], 
                                   one_layer$Count[3],
                                   one_layer$Count[4]))

one_layer_auc <- .9879

one_layer$Predicted <- factor(one_layer$Predicted, levels = c("Sank", "Not"))

ggplot(data = one_layer, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 11, family = "Optima") +
  scale_fill_manual(values = c("#65749D", "#D9D9D9", "#D9D9D9", "#65749D")) +
  labs(title = "\nOne-Layer Neural Network Confusion Matrix", 
       x = "Predicted", 
       y = "Actual", 
       fill = "", 
       subtitle = paste0("Accuracy: ", round(one_layer_accuracy,2), "%\nAUC: ", one_layer_auc)) +
  theme_transparent()  +
  theme(text = element_text(family = "Optima", size = 14),
        plot.subtitle = element_text(size = 12, hjust = 0.5),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "#e7f7fa")) +
  guides(fill = 'none')
```

## Two Layers
```{r}
two_layer <- data.frame(Predicted = c("Not", "Not", "Sank", "Sank"),
                        Actual = c("Not", "Sank", "Not", "Sank"),
                        Count = c(37948, 21, 50, 125))

two_layer_accuracy <- (two_layer$Count[1] + two_layer$Count[4]) / sum(two_layer$Count) * 100
two_layer_accuracy <- round(two_layer_accuracy, 3)
two_layer$Count <- factor(two_layer$Count, 
                        levels = c(two_layer$Count[1], 
                                   two_layer$Count[2], 
                                   two_layer$Count[3],
                                   two_layer$Count[4]))

two_layer_auc <- .9888
two_layer$Predicted <- factor(two_layer$Predicted, levels = c("Sank", "Not"))

ggplot(data = two_layer, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 11, family = "Optima") +
  scale_fill_manual(values = c("#65749D", "#D9D9D9", "#D9D9D9", "#65749D")) +
  labs(title = "\nTwo-Layer Neural Network Confusion Matrix", 
       x = "Predicted", 
       y = "Actual", 
       fill = "", 
       subtitle = paste0("Accuracy: ", round(two_layer_accuracy,2), "%\nAUC: ", two_layer_auc)) +
  theme_transparent()  +
  theme(text = element_text(family = "Optima", size = 14),
        plot.subtitle = element_text(size = 12, hjust = 0.5),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "#e7f7fa")) +
  guides(fill = 'none')
```
## Three Layers
```{r}
three_layer <- data.frame(Predicted = c("Not", "Not", "Sank", "Sank"),
                        Actual = c("Not", "Sank", "Not", "Sank"),
                        Count = c(37948, 31, 74, 101))

three_layer_accuracy <- (three_layer$Count[1] + three_layer$Count[4]) / sum(three_layer$Count) * 100
three_layer_accuracy <- round(three_layer_accuracy, 3)
three_layer$Count <- factor(three_layer$Count, 
                        levels = c(three_layer$Count[1], 
                                   three_layer$Count[2], 
                                   three_layer$Count[3],
                                   three_layer$Count[4]))

three_layer_auc <- .9888
three_layer$Predicted <- factor(three_layer$Predicted, levels = c("Sank", "Not"))

ggplot(data = three_layer, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 11, family = "Optima") +
  scale_fill_manual(values = c("#65749D", "#D9D9D9", "#D9D9D9", "#65749D")) +
  labs(title = "\nThree-Layer Neural Network Confusion Matrix", 
       x = "Predicted", 
       y = "Actual", 
       fill = "", 
       subtitle = paste0("Accuracy: ", round(three_layer_accuracy,2), "%\nAUC: ", three_layer_auc)) +
  theme_transparent()  +
  theme(text = element_text(family = "Optima", size = 14),
        plot.subtitle = element_text(size = 12, hjust = 0.5),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "#e7f7fa")) +
  guides(fill = 'none')
```

